

    Please summarize the following comments for a youtube video.
    title: Lex Fridman Podcast Chatbot with LangChain Agents + GPT 3.5
    comments: ['Note thate this notebook can cost upto $10 to run in OpenAI costs', 'Hey James - really appreciate all the effort you put into these videos. You\'re helping a lot of us learn about LLMs and connectors like LangChain.\n\nSome things I wish you could expand on in your videos (I realize all of these require more work, but I think would be a huge help!)\n1. The writing/chalk board style is helpful, but takes time to write out and doesn\'t always seem like you get the results you want. I think using a simple PowerPoint/Slides to build a base for your instruction would be more helpful. I like how "Data Independent" does it. He uses basic slides which are quick to make but is better than scribbling. You can lay out document text, API screenshots, real-life examples better than being able to draw with Wacom. It\'s night and day better for helping people visualize. Then if you can use the Wacom tablet to draw on top of it, great. But slides help layout the information you want to talk about a head of time instead of writing on the fly. \n2. The workbooks are incredible, it would be awesome if you could link to the API documentation when you\'re using it. E.g. you know that "stuff" or "map_reduce" are available for RetrievalQA.from_chain_type.chain_type.....why? Where are you looking? It\'s the old "teach a man to fish" analogy. Instead of people just copying what you\'re doing, they would be able to better change things or adapt to their own situation.\n3. I know you covered it in your other videos, but would have been great to talk about token limits in this Agent use case. Talking about putting in the prompt, the chat history, the retrieved documents from the vector DB, the query, and the answer all have to fit in your LLM token limit (I think). This is crucial info. It also affects the ConversationBufferWindowMemory "k=5".\n4. It would be helpful to talk about why Agent is even needed in this case. It looks like in your example it either does 1) "I don\'t know" or 2) use the Lex Fridman DB. Essentially that means it uses Lex Fridman DB every time, which doesn\'t help show why an Agent is needed. I know you only have so much time in a video to explain things but that\'s kind of crucial when setting up an Agent to decide across many different services.\n\nThank you again.', "That's very useful, thanks. How would you use this on a web api ? The memory/history would have to be manually recreated from the list of messages ? Do you have a sample for this ? Thanks", 'could you make a video explainig how to create a chatbot web page usign custom data as context for the LLM without having to reload all the data on every request (maybe with pinecone) and where to deploy it with an interface and not just colab notebooks?', 'So awesome! im gonna use your approach on my own kb. i just have one question - what is the advantage of ‘managed’ vector stores like pinecone over something like llama index where the index json file is generated and stored in my local machine. from data privacy perspective, llama index seems to make more sense. Basically what do i tell my manager if he asks why we should pay for pinecone when we can use open source vector store like llama index? cheers']
    summary: